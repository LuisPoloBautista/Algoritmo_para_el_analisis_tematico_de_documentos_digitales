{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Algoritmo de modelación de tópicos de documentos digitales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autores\n",
    "\n",
    "#### Luis Roberto Polo Bautista\n",
    "#### Karen Vanessa Martínez Acevedo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El objetivo del algoritmo es asignar áreas temáticasa de documentos digitales en PDF, que sirva como herramienta de apoyo al análisis temático dentro de la organización de la información,  con el fin de ser implementado en el desarrollo de vocabularios controlados, como: taxonomías, tesauros, ontologías, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversióndel texto completo de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En esta parte se utilizó el módulo PyMuPDF de Python, que entre otras funciones permite la identificación y conversión del texto de diferentes tipos de documentos a archivos editables. Es importante señalar que, el funcionamiento de este algoritmo toma como base documentos en  formato  PDF,  con  el  módulo PyMuPDF se  hace  uso  del  Reconocimiento  Óptico  de Caracteres,  que  permita  generar  archivos de texto  simple  (archivos  con  extensión  txt)  que contengan el texto completo de los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar los módulos necesario\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el documentoen formato PDF\n",
    "documento_pdf=\"c:/\"#<<<<<<<<-----Escribe la ruta donde se encuentra el documento PDF----->>>>>>>>\n",
    "# Leer el documento mediante el módulo PyMuPDF\n",
    "documento=fitz.open(documento_pdf)\n",
    "# Mostrar el número de páginas y los datos bibliográficos\n",
    "print (\"Número de páginas del documento: \",documento.pageCount)\n",
    "print (\"Datos bibligráficos: \",documento.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar un variable que contenga el nombre del nuevo archivo en formato txt\n",
    "documento_txt=open(documento_pdf + \".txt\", \"wb\")\n",
    " \n",
    "for paginas in documento: # Para todas las páginas del documento  hacer: \n",
    "    texto=paginas.getText().encode(\"utf8\") # Conversión del texto en formato UTF-8\n",
    "    documento_txt.write(texto) # Escribir el texto en un archivo con la extensión definida\n",
    "    documento_txt.write(b\"\\n-----\\n\")# Definir dentro del archivo las divisiones de cada página\n",
    "documento_txt.close() #Cerrar el documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificación de temas relevantes y modelación visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En  la  sección  de  identificación  de  temas se  utilizaron  los  módulos Numpy  y Scikit-Learn para el cálculo de frecuencia y vectores de palabrasy el modelo de Asignación Latente de Dirichlet (ALD) para la identificación de los temas relevantes. Para la visualización dinámica de los temaspor medio de HTML se utilizóel módulo pyLDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar los módulos necesarios\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import os\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el archivo de texto simple con el preprocesamiento realizado \n",
    "with open ('c:/Users/Luis/Desktop/book2.pdf.txt', 'r', encoding=\"utf-8\") as archivo: #<<<<<<<<-----Escribe la ruta donde se encuentra el documento txt----->>>>>>>>\n",
    "    texto= archivo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar los artículos, pronombres, preposiciones, etc., del texto \n",
    "agregar = ['No', 'text', 'returned', 'unable', 'to', 'reach', 'website.', 'one', 'new', '--', 'us'] #<<<<<<<<-----Escribe las palabras que no quiere que sean consideradas a parte de las palabras vacías----->>>>>>>>\n",
    "stop_words= (stopwords.words('spanish')) # <<<<<<<<-----para un texto en inglés, reemplazar 'spanish' por 'english' ----->>>>>>>>\n",
    "stop_words.extend (agregar)\n",
    "word_tokens= word_tokenize(texto.lower())\n",
    "word_tokens= list(filter(lambda token: token not in string.punctuation, word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bolsa_palabras=[]\n",
    "for palabra in word_tokens: \n",
    "    if palabra not in stop_words:\n",
    "        filtro.append(palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar todas las formas de una misma palabra \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = [lemmatizer.lemmatize(w) for w in bolsa_palabras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular los vectores de las palabras individuales que conforman cada frase \n",
    "vectores_palabras = CountVectorizer()\n",
    "transformar_vectores = vectores_palabras.fit_transform(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una función que extraiga los vectores de las palabras y los guarde en una variable\n",
    "def mostrar_tópicos(model, vectores_palabras, n_top_words):\n",
    "    palabras = vectores_palabras.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([palabras[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:- 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar en una variable el número de temas a procesas\n",
    "número_tópicos = 4 #<<<<<<<<-----Escribe el número de temas a mostrar----->>>>>>>>\n",
    "# Declarar en una variable el número de palabras relacionadas a los temas\n",
    "número_palabras = 30 #<<<<<<<<-----Escribe el número de palabras a considerar----->>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar los datos ingresados con base a modulo ALD\n",
    "lda = LDA(n_components=número_tópicos, n_jobs=-1)\n",
    "lda.fit(transformar_vectores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los datos procesados con base al vector asignado a cada palabra, así como el número de palabras relacionados a los tópicos \n",
    "print(\"Tópicos encontrados mediante ALD:\")\n",
    "mostrar_tópicos(lda, vectores_palabras, número_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar en una variable el nombre del archivo que mostrara los temas de forma dinámica \n",
    "LDAvis_data_filepath = os.path.join('./tópic_'+str(número_tópicos))#<<<<<<<<-----Escribe el nombre del archivo----->>>>>>>>\n",
    "# Procesar los datos para que se puedan mostrar de forma dinámica\n",
    "if 1 == 1:LDAvis_prepared = sklearn_lda.prepare(lda, transformar_vectores, vectores_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar el archivo con los datos a mostrar\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "with open(LDAvis_data_filepath, 'rb',) as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el archivo en una ruta determinada con extensión HTML\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'c:/'+ str(LDAvis_data_filepath) +'.html')#<<<<<<<<-----Escribe la ruta donde se quiere guardar el archivo HTML---->>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
