{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Algoritmo para el análisis tematico de documentos digitales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Luis Roberto Polo Bautista\n",
    "#### Karen Vanessa Martínez Acevedo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El objetivo del algoritmo es la modelación de temas de documentos digitales en español e inglés, que sirva como herramienta de apoyo al análisis temático dentro de la organización de la información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción del texto completo de documentos en formato PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En esta parte se utilizó el módulo PyMuPDF de Python, el cual, entre otras funciones permite la identificación y extracción del texto de diferentes tipos de documentos exportándolos a archivos editables. En función al objetivo del articulo, se implementó este módulo para importar documentos en formato pdf, extraer el texto de este y exportándolo en un archivo de texto simple (archivo con extensión txt). A continuación, se muestra el pseudo-código que se utilizó en este proceso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar los módulos necesario\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el documento de texto en formato PDF\n",
    "documento_pdf=\"c:/\"#<-----Escribe la ruta donde se encuentra el documento PDF\n",
    "# Leer el documento mediante el módulo PyMuPDF\n",
    "documento=fitz.open(documento_pdf)\n",
    "# Mostrar el número de páginas y los datos bibliográficos\n",
    "print (\"Número de páginas del documento: \",documento.pageCount)\n",
    "print (\"Datos bibligráficos: \",documento.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar un variable que contenga la extensión del nuevo archivo en formato txt\n",
    "documento_txt=open(documento_pdf + \".txt\", \"wb\")\n",
    " \n",
    "for paginas in documento: # Para todas las páginas del documento  hacer: \n",
    "    texto=paginas.getText().encode(\"utf8\") # Extracción del texto en formato UTF-8\n",
    "    documento_txt.write(texto) # Exportar el texto en un archivo con la extensión definida\n",
    "    documento_txt.write(b\"\\n-----\\n\")# Definir dentro del archivo las divisiones de cada página\n",
    "documento_txt.close() #Cerrar el documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelación de tópicos relevantes visualizándolos en HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En la sección de identificación de tópicos se utilizaron los módulos numpy y sklearn para el cálculo de frecuencia y vectores de palabras y el modelo de Asignación Latente de Dirichlet (ALD) para la identificación de los tópicos relevantes. Para la visualización dinámica de los temas por medio de html se utilizó el módulo pyLDAvis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar los módulos necesarios\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import os\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el archivo de texto simple con el preprocesamiento realizado \n",
    "with open ('c:/.pdf.txt', 'r', encoding=\"utf-8\") as archivo: #<-----Escribe la ruta donde se encuentra el documento txt\n",
    "    texto= archivo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar los artículos, pronombres, preposiciones, etc., del texto \n",
    "agregar = ['No', 'text', 'returned', 'unable', 'to', 'reach', 'website.', 'one', 'new', '--', 'us'] #<---Escribe las palabras que no quiere que sean consideradas a parte de las palabras vacías\n",
    "stop_words= (stopwords.words(spanish)) # para un texto en inglés, reemplazar 'spanish' por 'english'\n",
    "stop_words.extend (agregar)\n",
    "word_tokens= word_tokenize(texto.lower())\n",
    "word_tokens= list(filter(lambda token: token not in string.punctuation, word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro=[]\n",
    "for palabra in word_tokens: \n",
    "    if palabra not in stop_words:\n",
    "        filtro.append(palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizar todas las formas de una misma palabra \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = [lemmatizer.lemmatize(w) for w in filtro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular los vectores de las palabras individuales que conforman cada frase \n",
    "vectores_palabras = CountVectorizer()\n",
    "transformar_vectores = vectores_palabras.fit_transform(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una función que extraiga los vectores de las palabras y los guarde en una variable\n",
    "def mostrar_tópicos(model, vectores_palabras, n_top_words):\n",
    "    palabras = vectores_palabras.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([palabras[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:- 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar en una variable el número de tópicos a procesas\n",
    "número_tópicos = 4\n",
    "# Declarar en una variable el número de palabras relacionadas a los tópicos \n",
    "número_palabras = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar los datos ingresados con base a modulo LDA\n",
    "lda = LDA(n_components=número_tópicos, n_jobs=-1)\n",
    "lda.fit(transformar_vectores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los datos procesados con base al vector asignado a cada palabra, así como el número de palabras relacionados a los tópicos \n",
    "print(\"Tópicos encontrados mediante LDA:\")\n",
    "mostrar_tópicos(lda, vectores_palabras, número_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarar en una variable el nombre del archivo que mostrara los temas de forma dinámica \n",
    "LDAvis_data_filepath = os.path.join('./tópic_'+str(número_tópicos))#<---- Escribe el nomnbre del archivo\n",
    "# Procesar los datos para que se puedan mostrar de forma dinámica\n",
    "if 1 == 1:LDAvis_prepared = sklearn_lda.prepare(lda, transformar_vectores, vectores_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar el archivo con los datos a mostrar\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "with open(LDAvis_data_filepath, 'rb',) as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el archivo en una ruta determinada con extensión HTML\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'c:/'+ str(LDAvis_data_filepath) +'.html')#<-----Escribe la ruta donde se quiere guardar el archivo HTML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
